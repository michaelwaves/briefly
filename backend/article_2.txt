Skip to main content


EN

English Español Português Deutsch Beta Français Beta

More Information Found an Error?

Home
Tutorials
Artificial Intelligence
Transformer Model Tutorial in PyTorch: From Theory to Code
Learn how to build a Transformer model using PyTorch, a powerful tool in modern machine learning.

Updated Apr 10, 2025 · 15 minread

Training more people?
Get your team access to the full DataCamp for business platform.

For Business For a bespoke solution book a demo .

The Transformer is one of the most powerful models in modern machine learning. It has revolutionized the field, particularly in Natural Language Processing (NLP) tasks such as language translation and text summarization. Transformers have replaced Long-Short-Term Memory (LSTM) networks in these tasks due to their ability to handle long-range dependencies and parallel computations. PyTorch, a popular open-source machine learning library known for its simplicity, versatility, and efficiency, has become a go-to for researchers and developers in machine learning and artificial intelligence. This tutorial aims to provide a comprehensive understanding of how to construct a Transformer model using PyTorch. For those unfamiliar with PyTorch, a visit to DataCamp's course Efficient AI Model Training With PyTorch is recommended for a solid grounding.

Develop AI Applications
Learn to build AI applications using the OpenAI API.

Start Upskilling For Free

Transformers Background and Theory First introduced in the paper Attention is All You Need by Vaswani et al., Transformers have since become a cornerstone of many NLP tasks due to their unique design and effectiveness. At the heart of Transformers is the attention mechanism, specifically the concept of 'self-attention,' which allows the model to weigh and prioritize different parts of the input data. This mechanism enables Transformers to manage long-range dependencies in data. It is fundamentally a weighting scheme that allows a model to focus on different parts of the input when producing an output. The self-attention mechanism allows the model to consider different words or features in the input sequence, assigning each one a 'weight' that signifies its importance for producing a given output. For instance, in a sentence translation task, while translating a particular word, the model might assign higher attention weights to words that are grammatically or semantically related to the target word. This process allows the Transformer to capture dependencies between words or features, regardless of their distance from each other in the sequence. Transformers' impact in the field of NLP cannot be overstated. They have outperformed traditional models in many tasks, demonstrating a superior capacity to comprehend and generate human language in a more nuanced way. For a deeper understanding of NLP, DataCamp's Introduction to Natural Language Processing in Python course is a recommended resource. ## Setting up PyTorch Before building a Transformer, it is essential to set up the working environment correctly. First and foremost, PyTorch needs to be installed. PyTorch be installed through the pip or conda package managers. For pip, use the command: ```
pip3 install torch torchvision torchaudio For conda, use the command: conda install pytorch torchvision -c pytorch

2. Defining the basic building blocks: Multi-head Attention, Position-Wise Feed-Forward Networks, Positional Encoding.
3. Building the Encoder block.
4. Building the Decoder block.
5. Combining the Encoder and Decoder layers to create the complete Transformer network.
 ### 1\. Importing the necessary libraries and modules We’ll start with importing the PyTorch library for core functionality, the neural network module for creating neural networks, the optimization module for training networks, and the data utility functions for handling data. Additionally, we’ll import the standard Python `math` module for mathematical operations and the `copy` module for creating copies of complex objects. These tools set the foundation for defining the model's architecture, managing data, and establishing the training process. ```
import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import math
import copy
``` ### 2\. Defining the basic building blocks: Multi-Head Attention, Position-wise Feed-Forward Networks, Positional Encoding Before we get into building our components, take a look at the following table, which describes the different components of a Transformer and their purpose: |Component |Description |Purpose |
| --- | --- | --- |
|Multi-Head Attention |Mechanism to focus on different parts of the input |Captures dependencies across different positions in the sequence |
|Feed-Forward Networks |Position-wise fully connected layers |Transforms the attention outputs, adding complexity |
|Positional Encoding |Adds positional information to embeddings |Provides sequence order context to the model |
|Layer Normalization |Normalizes inputs to each sub-layer |Stabilizes training, improves convergence |
|Residual Connections |Shortcuts between layers |Helps in training deeper networks by minimizing gradient issues |
|Dropout |Randomly zeroes some network connections |Prevents overfitting by regularizing the model |
 #### Multi-head attention The multi-head attention mechanism computes the attention between each pair of positions in a sequence. It consists of multiple “attention heads” that capture different aspects of the input sequence. To learn more about multi-head attention, check out the [attention](https://campus.datacamp.com/courses/large-language-models-llms-concepts/training-methodology-and-techniques?ex=8#) [mechanisms](https://campus.datacamp.com/courses/large-language-models-llms-concepts/training-methodology-and-techniques?ex=8#) section of the Large Language Models (LLMs) Concepts course. _Figure 1. Multi-Head Attention (source: image created by author)_ ```
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        # Ensure that the model dimension (d_model) is divisible by the number of heads
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        
        # Initialize dimensions
        self.d_model = d_model # Model's dimension
        self.num_heads = num_heads # Number of attention heads
        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value
        
        # Linear layers for transforming inputs
        self.W_q = nn.Linear(d_model, d_model) # Query transformation
        self.W_k = nn.Linear(d_model, d_model) # Key transformation
        self.W_v = nn.Linear(d_model, d_model) # Value transformation
        self.W_o = nn.Linear(d_model, d_model) # Output transformation
        
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        # Calculate attention scores
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        # Apply mask if provided (useful for preventing attention to certain parts like padding)
        if mask is not None:
            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)
        
        # Softmax is applied to obtain attention probabilities
        attn_probs = torch.softmax(attn_scores, dim=-1)
        
        # Multiply by values to obtain the final output
        output = torch.matmul(attn_probs, V)
        return output
        
    def split_heads(self, x):
        # Reshape the input to have num_heads for multi-head attention
        batch_size, seq_length, d_model = x.size()
        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)
        
    def combine_heads(self, x):
        # Combine the multiple heads back to original shape
        batch_size, _, seq_length, d_k = x.size()
        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)
        
    def forward(self, Q, K, V, mask=None):
        # Apply linear transformations and split heads
        Q = self.split_heads(self.W_q(Q))
        K = self.split_heads(self.W_k(K))
        V = self.split_heads(self.W_v(V))
        
        # Perform scaled dot-product attention
        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # Combine heads and apply output transformation
        output = self.W_o(self.combine_heads(attn_output))
        return output
``` **Class definition and initialization:** ```
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
``` The class is defined as a subclass of PyTorch's `nn.Module` . 1. `d_model` : Dimensionality of the input.
2. `num_heads` : The number of attention heads to split the input into.
 The initialization checks if `d_model` is divisible by `num_heads` , and then defines the transformation weights for `query` , `key` , `value` , and `output` . **Scaled dot-product attention:** ```
def scaled_dot_product_attention(self, Q, K, V, mask=None):
``` 1. **Calculating attention scores** : `attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)` . Here, the attention scores are calculated by taking the dot product of queries ( `Q` ) and keys ( `K` ), and then scaling by the square root of the key dimension ( `d_k` ).
2. **Applying mask** : If a mask is provided, it is applied to the attention scores to mask out specific values.
3. **Calculating attention weights** : The attention scores are passed through a softmax function to convert them into probabilities that sum to 1.
4. **Calculating output** : The final output of the attention is calculated by multiplying the attention weights by the values ( `V` ).
 **Splitting heads:** ```
def split_heads(self, x):
``` This method reshapes the input x into the shape ( `batch_size` , `num_heads` , `seq_length` , `d_k` ). It enables the model to process multiple attention heads concurrently, allowing for parallel computation. **Combining heads:** ```
def combine_heads(self, x):
``` After applying attention to each head separately, this method combines the results back into a single tensor of shape ( `batch_size` , `seq_length` , `d_model` ). This prepares the result for further processing. **Forward method:** ```
def forward(self, Q, K, V, mask=None):
``` The forward method is where the actual computation happens: 1. **Apply linear transformations** : The queries ( `Q` ), keys ( `K` ), and values ( `V` ) are first passed through linear transformations using the weights defined in the initialization.
2. **Split heads** : The transformed `Q` , `K` , `V` are split into multiple heads using the `split_heads` method.
3. **Apply scaled dot-product attention** : The `scaled_dot_product_attention` method is called on the split heads.
4. **Combine heads** : The results from each head are combined back into a single tensor using the `combine_heads` method.
5. **Apply output transformation** : Finally, the combined tensor is passed through an output linear transformation.
 In summary, the `MultiHeadAttention` class encapsulates the multi-head attention mechanism commonly used in transformer models. It takes care of splitting the input into multiple attention heads, applying attention to each head, and then combining the results. By doing so, the model can capture various relationships in the input data at different scales, improving the expressive ability of the model. #### Position-wise feed-forward networks ```
class PositionWiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff):
        super(PositionWiseFeedForward, self).__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.relu = nn.ReLU()

    def forward(self, x):
        return self.fc2(self.relu(self.fc1(x)))
``` **Class definition:** ```
class PositionWiseFeedForward(nn.Module):
``` The class is a subclass of PyTorch's `nn.Module` , which means it will inherit all functionalities required to work with neural network layers. **Initialization:** ```
def __init__(self, d_model, d_ff):
    super(PositionWiseFeedForward, self).__init__()
    self.fc1 = nn.Linear(d_model, d_ff)
    self.fc2 = nn.Linear(d_ff, d_model)
    self.relu = nn.ReLU()
``` 1. `d_model` : Dimensionality of the model's input and output.
2. `d_ff` : Dimensionality of the inner layer in the feed-forward network.
3. `self.fc1` and `self.fc2` : Two fully connected (linear) layers with input and output dimensions as defined by `d_model` and `d_ff` .
4. `self.relu` : ReLU (Rectified Linear Unit) activation function, which introduces non-linearity between the two linear layers.
 **Forward Method:** ```
def forward(self, x):
    return self.fc2(self.relu(self.fc1(x)))
``` 1. `x` : The input to the feed-forward network.
2. `self.fc1(x)` : The input is first passed through the first linear layer ( `fc1` ).
3. `self.relu(...)` : The output of `fc1` is then passed through a ReLU activation function. ReLU replaces all negative values with zeros, introducing non-linearity into the model.
4. `self.fc2(...)` : The activated output is then passed through the second linear layer ( `fc2` ), producing the final output.
 In summary, the `PositionWiseFeedForward` class defines a position-wise feed-forward neural network that consists of two linear layers with a ReLU activation function in between. In the context of transformer models, this feed-forward network is applied to each position separately and ident
